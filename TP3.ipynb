{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roIHPPYGLCci"
      },
      "source": [
        "# Inicio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CCIkGi82Aj4",
        "outputId": "ed5b23aa-f5b3-464e-aa19-4e18b1e3c992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-05adrs0q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-05adrs0q\n",
            "  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.20)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "import whisper\n",
        "!pip install llama-cpp-python\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7_eO8pF2aI3",
        "outputId": "15921577-44a7-483a-c554-100185be60d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "BYG4E9WiL-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJUDBBw4K1bt"
      },
      "source": [
        "# Moroi (Spirit Box, Temperaduras Heladas, Escritura Fantasma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_whisper_options = {\"language\": \"spanish\"}\n",
        "out = model.transcribe(\"/content/drive/MyDrive/Colab Notebooks/phasmophobia_moroi.mp3\", **other_whisper_options)\n",
        "evidencia_text = out[\"text\"]\n",
        "evidencia_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Jf8az4PeMJcG",
        "outputId": "1e2a8d1e-ef4d-46a4-e73e-97f2964274aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Listo, ya empecé a grabar. Voy a agarrar la linterna y voy a entrar a la casa. Abro la puerta. Uy, se movió algo. Acá es la habitación, la cocina. Bueno, voy a empezar a traer evidencias. Voy a probar con esta y... Por ahora no tengo nada. Bueno... Uy, está escribiendo el libro. Tenemos escritura fantasma. Bien. Voy a intentar hablarle. Hello, are you here? Ahí me respondió. Entonces, hay spirit box también. No falta la última evidencia. Bueno, vamos a esperar un rato para tener la última evidencia. Bueno, no está pasando nada. Me voy a esconder antes de que empiece a atacar. Uy, está atacando. No hay ruido. No hay ruido. Salgo. Termino de atacar. Prendo la luz y tomo la temperatura y está bajo cero. Perfecto. Nos vamos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFXqiZcr5fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a504e673-c806-490b-bde0-5f51ee77ae7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'llama.cpp'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcEJUXPPr6RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b6cac8-c74c-440d-b363-291ac4f7c2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make: *** empty variable name.  Stop.\n"
          ]
        }
      ],
      "source": [
        "!make LLAMA_CUBLAS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE71636Gr_Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e3fbe5-7cad-4abc-8512-52660877d533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctransformers[cuda] in /usr/local/lib/python3.10/dist-packages (0.2.27)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.20)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11 in /usr/local/lib/python3.10/dist-packages (11.8.89)\n",
            "Requirement already satisfied: nvidia-cublas-cu11 in /usr/local/lib/python3.10/dist-packages (11.11.3.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (0.19.4)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (9.0.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (12.3.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (12.3.101)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ctransformers[cuda] llama-cpp-python nvidia-cuda-runtime-cu11 nvidia-cublas-cu11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJxV0AmrsBMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98a120f-0e36-41aa-ac8b-300eb4e12e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 14:20:57--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.118, 65.8.178.27, 65.8.178.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1702045258&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjA0NTI1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FmcPqyTsIMBHU0a9puOh91VFvJilEDCJQBTioNaZVBDK3FSjyO0-Jm7Q8WnktY6G2CmsxKGOMc2cICPYtk3l1oNNKnUOPyk8YBke5PShyTop6FeWFyBhmsYyUTs-jCGJWyRmNTAycFPhpO7dyF0wlzF3rhwjK0n6MFmm98HPcHX9mYbTTFgXW2eU3Jbi8uh3nEgSCRemiWKBiDh50wKHOeEewds9vnFsttYXGWhczgaCRWK9hfxEQK0TKeo%7EuPksxETFqBvh4loJUgyHNzA6srEhfvSQAuQNF5RylpoNdWMo7NJjLiW43L8F1HJJbPmQZH4WCEnYqzxSfl9vXt2OhA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-05 14:20:58--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1702045258&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjA0NTI1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FmcPqyTsIMBHU0a9puOh91VFvJilEDCJQBTioNaZVBDK3FSjyO0-Jm7Q8WnktY6G2CmsxKGOMc2cICPYtk3l1oNNKnUOPyk8YBke5PShyTop6FeWFyBhmsYyUTs-jCGJWyRmNTAycFPhpO7dyF0wlzF3rhwjK0n6MFmm98HPcHX9mYbTTFgXW2eU3Jbi8uh3nEgSCRemiWKBiDh50wKHOeEewds9vnFsttYXGWhczgaCRWK9hfxEQK0TKeo%7EuPksxETFqBvh4loJUgyHNzA6srEhfvSQAuQNF5RylpoNdWMo7NJjLiW43L8F1HJJbPmQZH4WCEnYqzxSfl9vXt2OhA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.99, 108.157.162.95, 108.157.162.58, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9229924224 (8.6G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q5_K_M.gguf.1’\n",
            "\n",
            "llama-2-13b-chat.Q5 100%[===================>]   8.60G  36.8MB/s    in 3m 26s  \n",
            "\n",
            "2023-12-05 14:24:24 (42.8 MB/s) - ‘llama-2-13b-chat.Q5_K_M.gguf.1’ saved [9229924224/9229924224]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"[INST] <<SYS>>Un sistema de clasificación de textos. Se clasifica -evidencia: Tres valores de la lista [Temperaturas Heladas, Escritura Fantasma, Spirit Box, Proyector D.O.T.S, Ultravioleta, Orbes Espectrales, Medidor EMF 5]. Tienes que determinar la clasificación del siguiente texto basandote en si este contiene referencias a alguna de estas siete evidencias. El sistema no genera explicaciones de los resultados, solo los presenta en el formato indicado. Solo escribe las tres evidencias y para. Ejemplo 1: 'Tengo el termómetro y este me dice que... La temperatura es bajo cero. También hay medidas EMF 5 y veo orbes en la cámara.' Clasificación: [Temperaturas Heladas, Medidor EMF 5, Orbes Espectrales]. Ejemplo 2: 'Estoy viendo... Veo orbes. Veo ultravioleta en la puerta también. Ah, mira, también hay DOTS.' Clasificación: [Orbes Espectrales, Ultravioleta, Proyector D.O.T.S]. Ejemplo 3: 'Me respondió el fantasma, hay spirit box. Espera, está escribiendo. Hay escritura fantasma también. Además de los orbes que vimos.' Clasificación: [Spirit Box, Escritura Fantasma, Orbes Espectrales]. El texto a clasificar se delimita por ::: . :::\" + evidencia_text + \":::<</SYS>> Escribe la clasificación en formato json.[/INST] Clasificación:\"\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "OBrbw4YOlW80",
        "outputId": "e21cb282-0e5d-4569-bb8a-7c458a8c53ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST] <<SYS>>Un sistema de clasificación de textos. Se clasifica -evidencia: Tres valores de la lista [Temperaturas Heladas, Escritura Fantasma, Spirit Box, Proyector D.O.T.S, Ultravioleta, Orbes Espectrales, Medidor EMF 5]. Tienes que determinar la clasificación del siguiente texto basandote en si este contiene referencias a alguna de estas siete evidencias. El sistema no genera explicaciones de los resultados, solo los presenta en el formato indicado. Solo escribe las tres evidencias y para. Ejemplo 1: 'Tengo el termómetro y este me dice que... La temperatura es bajo cero. También hay medidas EMF 5 y veo orbes en la cámara.' Clasificación: [Temperaturas Heladas, Medidor EMF 5, Orbes Espectrales]. Ejemplo 2: 'Estoy viendo... Veo orbes. Veo ultravioleta en la puerta también. Ah, mira, también hay DOTS.' Clasificación: [Orbes Espectrales, Ultravioleta, Proyector D.O.T.S]. Ejemplo 3: 'Me respondió el fantasma, hay spirit box. Espera, está escribiendo. Hay escritura fantasma también. Además de los orbes que vimos.' Clasificación: [Spirit Box, Escritura Fantasma, Orbes Espectrales]. El texto a clasificar se delimita por ::: . ::: Listo, ya empecé a grabar. Voy a agarrar la linterna y voy a entrar a la casa. Abro la puerta. Uy, se movió algo. Acá es la habitación, la cocina. Bueno, voy a empezar a traer evidencias. Voy a probar con esta y... Por ahora no tengo nada. Bueno... Uy, está escribiendo el libro. Tenemos escritura fantasma. Bien. Voy a intentar hablarle. Hello, are you here? Ahí me respondió. Entonces, hay spirit box también. No falta la última evidencia. Bueno, vamos a esperar un rato para tener la última evidencia. Bueno, no está pasando nada. Me voy a esconder antes de que empiece a atacar. Uy, está atacando. No hay ruido. No hay ruido. Salgo. Termino de atacar. Prendo la luz y tomo la temperatura y está bajo cero. Perfecto. Nos vamos.:::<</SYS>> Escribe la clasificación en formato json.[/INST] Clasificación:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmDv8m9Gs6HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32601582-7f96-4193-e761-eb449f052b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-2ca6b154-26fc-4fd5-8e86-b9f067bdbfe3', 'object': 'text_completion', 'created': 1701786320, 'model': '/content/llama-2-13b-chat.Q5_K_M.gguf', 'choices': [{'text': ' [Escritura Fantasma, Spirit Box, Temperaturas Heladas]', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 637, 'completion_tokens': 18, 'total_tokens': 655}}\n"
          ]
        }
      ],
      "source": [
        "llama = Llama(model_path = \"/content/llama-2-13b-chat.Q5_K_M.gguf\", n_gpu_layers = 1, n_ctx = 1024)\n",
        "generation = llama(prompt , max_tokens = 40, stop = [\"[INST]\"], echo = False)\n",
        "\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUisVb29u4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca790c3c-cf77-47d5-e7e5-8a14315bd7af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' [Escritura Fantasma, Spirit Box, Temperaturas Heladas]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "generated_text = generation[\"choices\"][0][\"text\"]\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"Orbes Espectrales\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Banshee\")\n",
        "\n",
        "if \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Demonio\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Deogen\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Goryo\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Hantu\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Jinn\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Pesadilla\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Moroi\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Myling\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Orbes Espectrales\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Obake\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Oni\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text:\n",
        "    print(\"El fantasma es: Onryo\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Ente\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Poltergeist\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Orbes Espectrales\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Raiju\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Revenant\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Sombra\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Espíritu\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Escritura Fantasma\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Thaye\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Mímico\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text:\n",
        "    print(\"Los fantasmas son: Gemelos\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"Los fantasmas son: Espectro\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Yokai\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Yurei\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwEUIIMv4Aq0",
        "outputId": "3f558014-cf75-49ff-de6a-08218e8f4251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El fantasma es: Moroi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raiju (Medidor EMF 5, Orbes Espectrales, Proyector D.O.T.S"
      ],
      "metadata": {
        "id": "w42h_8DTXDz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "other_whisper_options = {\"language\": \"spanish\"}\n",
        "out = model.transcribe(\"/content/drive/MyDrive/Colab Notebooks/phasmophobia_raiju.mp3\", **other_whisper_options)\n",
        "evidencia_text = out[\"text\"]\n",
        "evidencia_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4psxx_DgXVck",
        "outputId": "9268feab-7fbb-4289-fef4-3a6a178f4b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Estoy entrando al asilo. Está abandonado obviamente, está todo oscuro. Voy a ir a prender la luz. Bueno acá no está. Sobre esta la otra habitación. Acá listo. Ya puedo desprender la luz. Acá hay vapor. Hay vapor. Así que acá está el fantasma. Estoy por probar las evidencias. Veamos, está sonando el EMF. Así que tengo medidor EMF5. EMF5. Bueno, ahora está pasando por el proyector DOTS. Perfecto. ¿Y qué nos falta? Nos falta una evidencia más. Vamos a ver si... voy a probar otra cosa más. A ver qué sale. A ver... Estoy encontrando acá. Con la cámara estoy viendo que hay orbes. Hay orbes. Listo. Ya podemos irnos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"[INST] <<SYS>>Un sistema de clasificación de textos. Se clasifica -evidencia: Tres valores de la lista [Temperaturas Heladas, Escritura Fantasma, Spirit Box, Proyector D.O.T.S, Ultravioleta, Orbes Espectrales, Medidor EMF 5]. Tienes que determinar la clasificación del siguiente texto basandote en si este contiene referencias a alguna de estas siete evidencias. El sistema no genera explicaciones de los resultados, solo los presenta en el formato indicado. Solo escribe las tres evidencias y para. Ejemplo 1: 'Tengo el termómetro y este me dice que... La temperatura es bajo cero. También hay medidas EMF 5 y veo orbes en la cámara.' Clasificación: [Temperaturas Heladas, Medidor EMF 5, Orbes Espectrales]. Ejemplo 2: 'Estoy viendo... Veo orbes. Veo ultravioleta en la puerta también. Ah, mira, también hay DOTS.' Clasificación: [Orbes Espectrales, Ultravioleta, Proyector D.O.T.S]. Ejemplo 3: 'Me respondió el fantasma, hay spirit box. Espera, está escribiendo. Hay escritura fantasma también. Además de los orbes que vimos.' Clasificación: [Spirit Box, Escritura Fantasma, Orbes Espectrales]. El texto a clasificar se delimita por ::: . :::\" + evidencia_text + \":::<</SYS>> Escribe la clasificación en formato json.[/INST] Clasificación:\"\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TZrQyJO5p7rQ",
        "outputId": "af75785e-e321-4152-caec-f3cd31ffe7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST] <<SYS>>Un sistema de clasificación de textos. Se clasifica -evidencia: Tres valores de la lista [Temperaturas Heladas, Escritura Fantasma, Spirit Box, Proyector D.O.T.S, Ultravioleta, Orbes Espectrales, Medidor EMF 5]. Tienes que determinar la clasificación del siguiente texto basandote en si este contiene referencias a alguna de estas siete evidencias. El sistema no genera explicaciones de los resultados, solo los presenta en el formato indicado. Solo escribe las tres evidencias y para. Ejemplo 1: 'Tengo el termómetro y este me dice que... La temperatura es bajo cero. También hay medidas EMF 5 y veo orbes en la cámara.' Clasificación: [Temperaturas Heladas, Medidor EMF 5, Orbes Espectrales]. Ejemplo 2: 'Estoy viendo... Veo orbes. Veo ultravioleta en la puerta también. Ah, mira, también hay DOTS.' Clasificación: [Orbes Espectrales, Ultravioleta, Proyector D.O.T.S]. Ejemplo 3: 'Me respondió el fantasma, hay spirit box. Espera, está escribiendo. Hay escritura fantasma también. Además de los orbes que vimos.' Clasificación: [Spirit Box, Escritura Fantasma, Orbes Espectrales]. El texto a clasificar se delimita por ::: . ::: Estoy entrando al asilo. Está abandonado obviamente, está todo oscuro. Voy a ir a prender la luz. Bueno acá no está. Sobre esta la otra habitación. Acá listo. Ya puedo desprender la luz. Acá hay vapor. Hay vapor. Así que acá está el fantasma. Estoy por probar las evidencias. Veamos, está sonando el EMF. Así que tengo medidor EMF5. EMF5. Bueno, ahora está pasando por el proyector DOTS. Perfecto. ¿Y qué nos falta? Nos falta una evidencia más. Vamos a ver si... voy a probar otra cosa más. A ver qué sale. A ver... Estoy encontrando acá. Con la cámara estoy viendo que hay orbes. Hay orbes. Listo. Ya podemos irnos.:::<</SYS>> Escribe la clasificación en formato json.[/INST] Clasificación:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama = Llama(model_path = \"/content/llama-2-13b-chat.Q5_K_M.gguf\", n_gpu_layers = 1, n_ctx = 1024)\n",
        "generation = llama(prompt , max_tokens = 40, stop = [\"[INST]\"], echo = False)\n",
        "\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5FG6TE4YpLS",
        "outputId": "8c29d562-bf5c-4316-e66f-dd4629a73e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-5ccab242-ce99-4116-9f1f-60009e886055', 'object': 'text_completion', 'created': 1701788581, 'model': '/content/llama-2-13b-chat.Q5_K_M.gguf', 'choices': [{'text': ' [Medidor EMF 5, Orbes Espectrales, Proyector D.O.T.S]', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 606, 'completion_tokens': 26, 'total_tokens': 632}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generation[\"choices\"][0][\"text\"]\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9VMcfjbAYzI7",
        "outputId": "6add04d7-d8c5-40c3-cd33-4e54a652f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' [Medidor EMF 5, Orbes Espectrales, Proyector D.O.T.S]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"Orbes Espectrales\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Banshee\")\n",
        "\n",
        "if \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Demonio\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Deogen\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Goryo\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Hantu\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Jinn\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Pesadilla\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Moroi\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Myling\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Orbes Espectrales\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Obake\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Oni\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text:\n",
        "    print(\"El fantasma es: Onryo\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Ultravioleta\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Ente\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Ultravioleta\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Poltergeist\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Orbes Espectrales\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Raiju\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Revenant\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Sombra\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Escritura Fantasma\" in generated_text:\n",
        "    print(\"El fantasma es: Espíritu\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Escritura Fantasma\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Thaye\")\n",
        "\n",
        "if \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Ultravioleta\" in generated_text:\n",
        "    print(\"El fantasma es: Mímico\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Temperaturas Heladas\" in generated_text:\n",
        "    print(\"Los fantasmas son: Gemelos\")\n",
        "\n",
        "if \"Medidor EMF 5\" in generated_text and \"Spirit Box\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"Los fantasmas son: Espectro\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Spirit Box\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Yokai\")\n",
        "\n",
        "if \"Orbes Espectrales\" in generated_text and \"Temperaturas Heladas\" in generated_text and \"Proyector D.O.T.S\" in generated_text:\n",
        "    print(\"El fantasma es: Yurei\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG4oN3mFY1sS",
        "outputId": "264106e4-99b7-4d38-9f4e-7eaba206156e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El fantasma es: Raiju\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}